{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f761a4-431e-4f51-993a-aa8218b55905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "model = ChatOpenAI(api_key=config[\"OPEN_AI_KEY\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812b79a8-ccc7-4c25-9b7b-92a1bb23cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph below contains a single node called \"oracle\" that executes a chat model, then returns the result\n",
    "\n",
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"oracle\", model)\n",
    "graph.add_edge(\"oracle\", END)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23823a6e-3b08-45bf-9229-00cfa5072e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 1 + 1?', id='83e8e5f0-6f31-4651-b036-fe7419ab5a85'),\n",
       " AIMessage(content='1 + 1 equals 2.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-45fcc772-760b-4abe-a0d5-542871ce2c01-0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "runnable.invoke(HumanMessage(\"What is 1 + 1?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f2e352-337d-4912-af79-ff5ae50117f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edges\n",
    "# Using tool calling\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number\n",
    "\n",
    "model = ChatOpenAI(api_key=config[\"OPEN_AI_KEY\"], temperature=0)\n",
    "model_with_tools = model.bind_tools([multiply])\n",
    "\n",
    "builder = MessageGraph()\n",
    "\n",
    "builder.add_node(\"oracle\", model_with_tools)\n",
    "\n",
    "tool_node = ToolNode([multiply])\n",
    "builder.add_node(\"multiply\", tool_node)\n",
    "\n",
    "builder.add_edge(\"multiply\", END)\n",
    "\n",
    "builder.set_entry_point(\"oracle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eceaa8c1-60aa-424d-83cb-7aa075f23bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can achieve this using conditional edges, which call a function on the current state and routes execution to a node the function's output\n",
    "\n",
    "from typing import Literal\n",
    "from typing import List\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "def router(state: List[BaseMessage]) -> Literal[\"multiply\", \"__end__\"]:\n",
    "    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "builder.add_conditional_edges(\"oracle\", router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a177ba59-0894-46d4-833e-adb43b1652ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 123 * 456?', id='a1630dc9-afa4-4a46-b89d-39e18863650e'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_LcOiiG7i5gRJvYvGo3RSjQNL', 'function': {'arguments': '{\"first_number\":123,\"second_number\":456}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 69, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7284c62f-a4e6-4849-95d7-c94998978331-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_LcOiiG7i5gRJvYvGo3RSjQNL'}]),\n",
       " ToolMessage(content='56088', name='multiply', id='06dfea89-efc7-4717-b06d-4ff26ab2fc38', tool_call_id='call_LcOiiG7i5gRJvYvGo3RSjQNL')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = builder.compile()\n",
    "\n",
    "runnable.invoke(HumanMessage(\"What is 123 * 456?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2419af-be62-4a04-948d-9424342d9647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is your name?', id='1a000182-b88b-4b76-adfe-7da0e210a021'),\n",
       " AIMessage(content='My name is Assistant. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 66, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cffdc2b5-37f1-4850-93cf-ef04953940c0-0')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the model output contains a tool call, we move to the \"multiply\" node. Otherwise, we end execution\n",
    "# Conversational responses are outputted directly\n",
    "\n",
    "runnable.invoke(HumanMessage(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de31350a-df46-41ee-9d51-0e2a34f0d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TAVILY_API_KEY=\"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\"\n"
     ]
    }
   ],
   "source": [
    "# Cycles\n",
    "\n",
    "# # DELETE THIS\n",
    "# %env TAVILY_API_KEY=\"tvly\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669e2478-0ae7-48cc-bab2-ae9be162b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap these tools in a simple LangGraph ToolNode\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944a40ce-f36f-474a-9f97-7ba7c238ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We will set streaming=True so that we can stream tokens\n",
    "# See the streaming section\n",
    "model = ChatOpenAI(api_key=config[\"OPEN_AI_KEY\"], model=\"gpt-3.5-turbo\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8337b6ec-12c7-45e7-a1cc-367f7ff8344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model knows that it has these tools available to call\n",
    "\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40c43f8-42aa-4c9f-a6d9-1929341b8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Add-don't-overwrite.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The `add_messages` function within the annotation defines\n",
    "    # *how* updates should be merged into the state.\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2304dc-cca4-4d0b-ae0d-7d07996e0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes\n",
    "\n",
    "###\n",
    "# Conditional Edge: after the agent is called, we should either:\n",
    "\n",
    "# a. Run tools if the agent said to take an action, OR\n",
    "\n",
    "# b. Finish (respond to the user) if the agent did not ask to run tools\n",
    "\n",
    "# Normal Edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\n",
    "###\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState) -> Literal[\"action\", \"__end__\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"action\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f9ca97-e568-4ae9-9a39-2a18dedd0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Graph\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('action', 'agent')\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe19705a-a18b-486d-a392-ff23271c8615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_w5yXd61MyyxSNeHkNPKAAw9A', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-3a27ea63-dc1f-4188-9b04-0efd96a6297d-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_w5yXd61MyyxSNeHkNPKAAw9A'}]),\n",
       "  ToolMessage(content=\"HTTPError('400 Client Error: Bad Request for url: https://api.tavily.com/search')\", name='tavily_search_results_json', tool_call_id='call_w5yXd61MyyxSNeHkNPKAAw9A'),\n",
       "  AIMessage(content='I encountered an error while trying to fetch the weather information for San Francisco. Let me try again.', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_KZvCK1zNa1vjPLhuHeErIJAO', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-fd08c224-6090-4473-b2bd-8dc48d4c8af4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_KZvCK1zNa1vjPLhuHeErIJAO'}]),\n",
       "  ToolMessage(content=\"HTTPError('400 Client Error: Bad Request for url: https://api.tavily.com/search')\", name='tavily_search_results_json', tool_call_id='call_KZvCK1zNa1vjPLhuHeErIJAO'),\n",
       "  AIMessage(content='I apologize for the inconvenience, but it seems there is an issue with retrieving the weather information for San Francisco at the moment.', response_metadata={'finish_reason': 'stop'}, id='run-04cf93bb-e4b9-4fba-b39d-68181be851be-0')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98550d-c2f1-4852-ae5d-178f567a94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

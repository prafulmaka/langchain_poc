{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6174a51-f385-420c-9658-9aafa0b44de7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "from dotenv import dotenv_values\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec93e65f-2454-471b-ba38-8520bca44ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b895924-33e7-4e6a-987a-540126e56260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T19:35:03.335294Z",
     "start_time": "2025-02-22T19:35:03.058974Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f357250f-3783-4d26-8d5d-1387a96d830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=config[\"OPEN_AI_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c355ecc9-5ad3-4891-84bd-ed87bd9bd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=\"testthiskey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0925b73e-138f-4800-8d80-d28d3cc92689",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715a831e-c1cf-47cb-99e6-ff1aae898d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99c45a6-134e-46cc-bb21-bb0a168d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c52771-0b9e-4e44-a941-b9f00202c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "290bd9d8-0e0f-4f9f-9ac2-c24a18502d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3aba018-5f61-4b09-81d1-1dfa17da9b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramaka/Desktop/Projects/LangChain/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9303e6de-2948-435e-8eec-8d2e8f82f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=config[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6663f2d0-3c2f-4d59-b887-5a2704ad24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d541f993-46c1-49bf-b882-d08c4086cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    # content = state['plan'] or []\n",
    "    content = []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af4cb00-8b16-4bcd-9ab5-1ed6ed7e986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af3c8dd1-4521-41f4-8d8a-0a087ee7000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61d1481-4ae6-4a30-83bd-d1736eb3b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97f1f80-2e7e-4209-a722-2a2286ff66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4e0b72-65ce-4983-8f31-f9ffe7569f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b0eb14-aba2-45da-afaf-6c19cae8ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1074cfe60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "609fff9e-9156-49dd-9a85-4d87c904a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1074cfe60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa6e2873-66fc-4303-acc6-3b8eb62fe068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1074cfe60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa3e8990-cd6a-45c2-9f0f-0abd1e7575a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1074cfe60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1782cfe-11d6-4f40-a11b-b860e518356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efc8172a-f668-4c3c-99a3-55793e5c216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'I. Introduction\\n    A. Brief overview of Langchain and Langsmith\\n    B. Thesis statement: Exploring the differences between Langchain and Langsmith\\n\\nII. Langchain\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIII. Langsmith\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIV. Comparison between Langchain and Langsmith\\n    A. Technology stack\\n    B. Scalability\\n    C. Security\\n    D. Interoperability\\n    E. Performance\\n\\nV. Conclusion\\n    A. Recap of main differences between Langchain and Langsmith\\n    B. Implications for the future of blockchain technology\\n    C. Final thoughts and recommendations\\n\\nNotes:\\n- Ensure a clear and concise explanation of both Langchain and Langsmith.\\n- Provide specific examples of use cases for each technology.\\n- Use comparative analysis to highlight the distinctions between Langchain and Langsmith.\\n- Conclude with insights on the potential impact of these technologies on the blockchain industry.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramaka/Desktop/Projects/LangChain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1377: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/Users/pramaka/Desktop/Projects/LangChain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'research_plan': {'content': ['LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain aids in implementing the retrieval-augmented generation (RAG) pattern in applications, simplifying data retrieval from various sources. LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). Indexes in LangChain serve as databases, organizing and storing information in a structured manner so relevant data is retrieved efficiently when the system processes language queries. LangChain offers a getting started example, to help you create an AI application in just a few minutes. By combining Langflow with LangChain, you accelerate your development process and more easily explore the capabilities of language models in your applications.', 'LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. What Is AWS? Developers on AWS', \"Langsmith is an innovative framework designed to enhance and streamline the development of natural language processing (NLP) applications. By leveraging *LangChain's* capabilities, Langsmith simplifies the process of chaining multiple language models and other NLP components to build powerful applications. By integrating LangChain, Langsmith allows developers to create intricate NLP workflows that can perform various tasks, from text generation to sentiment analysis. Introduction to Langsmith Langsmith is an innovative framework designed to enhance and streamline the development of natural language processing (NLP) applications. It provides a framework for connecting language models to other data sources and interacting with various APIs. LangChain is designed to be easy to use, even for developers who are not familiar with lang 13 min read\", 'Get started with LangSmith | 🦜️🛠️ LangSmith Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. LangSmith + LangChain OSS LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application. Get started by creating your first prompt.']}}\n",
      "{'generate': {'draft': \"**Title: LangChain vs LangSmith: A Comparative Analysis**\\n\\nI. Introduction\\nLangChain and LangSmith are two innovative tools developed by LangChain, aimed at simplifying the development and deployment of Large Language Model (LLM) applications. This essay will delve into the distinctions between LangChain and LangSmith, shedding light on their unique features and applications.\\n\\nII. LangChain\\nLangChain serves as a Python framework tailored to streamline AI application development, focusing on real-time data processing and integration with large language models. It simplifies the implementation of the retrieval-augmented generation (RAG) pattern, facilitating efficient data retrieval from diverse sources. LangChain is ideal for early-stage prototyping and small-scale applications, offering tools to connect language models with external data sources and customize prompt templates for various applications.\\n\\nIII. LangSmith\\nIn contrast, LangSmith is an innovative framework designed to enhance and streamline the development of natural language processing (NLP) applications. By leveraging LangChain's capabilities, LangSmith simplifies the process of chaining multiple language models and NLP components to build powerful applications. It provides a platform for building production-grade LLM applications, offering LLM-native observability features for monitoring and debugging applications at various stages.\\n\\nIV. Comparison between LangChain and LangSmith\\n- **Technology Stack:** LangChain focuses on real-time data processing and integration with LLMs, while LangSmith enhances NLP application development by connecting language models to various data sources.\\n- **Scalability:** LangChain is suitable for early-stage prototyping and small-scale applications, whereas LangSmith is geared towards large-scale, production-ready applications.\\n- **Security:** Both LangChain and LangSmith prioritize data security and offer tools to ensure the integrity of applications.\\n- **Interoperability:** LangChain and LangSmith can be integrated with external data sources and APIs, enhancing the versatility of applications.\\n- **Performance:** LangChain aids in implementing the RAG pattern efficiently, while LangSmith provides tools for prompt engineering and performance evaluation.\\n\\nV. Conclusion\\nIn conclusion, LangChain and LangSmith cater to distinct stages and requirements of LLM development, offering unique features and advantages. Understanding the differences between these tools is crucial for developers to choose the most suitable platform for their specific needs. The potential impact of LangChain and LangSmith on the blockchain industry lies in their ability to streamline AI application development and enhance the capabilities of language models in various applications. Embracing these technologies can lead to more efficient and advanced LLM applications in the future.\", 'revision_number': 2}}\n",
      "{'reflect': {'critique': \"**Critique:**\\n\\n1. **Introduction:**\\n   - The introduction provides a clear overview of the topic but could benefit from a more engaging hook to capture the reader's interest.\\n   - Consider adding a brief background on the significance of Large Language Models (LLMs) and the challenges in developing and deploying them.\\n\\n2. **Content:**\\n   - The essay effectively outlines the features and applications of LangChain and LangSmith, providing a comprehensive comparison between the two tools.\\n   - The comparison section is well-structured, highlighting key differences in technology stack, scalability, security, interoperability, and performance.\\n   - It would be beneficial to include specific examples or case studies to illustrate how each tool is used in real-world scenarios.\\n\\n3. **Depth and Analysis:**\\n   - While the essay covers the basics of LangChain and LangSmith, it could be enhanced by delving deeper into the technical aspects of each tool.\\n   - Consider discussing the underlying algorithms, architecture, and unique selling points of LangChain and LangSmith to provide a more detailed analysis.\\n\\n4. **Clarity and Coherence:**\\n   - The essay maintains a clear and logical flow, transitioning smoothly between sections.\\n   - Ensure that technical terms and concepts are explained in a way that is accessible to readers who may not be familiar with AI development or LLMs.\\n\\n5. **Conclusion:**\\n   - The conclusion effectively summarizes the key points discussed in the essay.\\n   - Consider reinforcing the significance of understanding the distinctions between LangChain and LangSmith for developers and the potential implications for the AI industry.\\n\\n**Recommendations:**\\n\\n1. **Expand on Technical Details:**\\n   - Provide a more in-depth analysis of the technical aspects of LangChain and LangSmith, including their architecture, algorithms, and key functionalities.\\n\\n2. **Include Real-World Examples:**\\n   - Incorporate case studies or examples to demonstrate how LangChain and LangSmith are used in practical applications, showcasing their effectiveness and versatility.\\n\\n3. **Enhance Engagement:**\\n   - Add a compelling introduction that sets the context for the discussion and captures the reader's attention from the outset.\\n\\n4. **Consider Future Implications:**\\n   - Discuss the potential future developments and impacts of LangChain and LangSmith on the AI industry, highlighting their role in advancing LLM applications.\\n\\n5. **Length and Detail:**\\n   - Consider expanding the essay to provide more detailed insights into the features, functionalities, and applications of LangChain and LangSmith.\\n\\n6. **Style and Tone:**\\n   - Maintain a professional and informative tone throughout the essay, ensuring clarity and precision in conveying complex technical information.\\n\\nBy incorporating these recommendations, you can enrich the analysis of LangChain and LangSmith, providing readers with a more comprehensive understanding of these innovative tools and their implications for AI development.\"}}\n",
      "{'research_critique': {'content': ['LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain aids in implementing the retrieval-augmented generation (RAG) pattern in applications, simplifying data retrieval from various sources. LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). Indexes in LangChain serve as databases, organizing and storing information in a structured manner so relevant data is retrieved efficiently when the system processes language queries. LangChain offers a getting started example, to help you create an AI application in just a few minutes. By combining Langflow with LangChain, you accelerate your development process and more easily explore the capabilities of language models in your applications.', 'LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. What Is AWS? Developers on AWS', \"Langsmith is an innovative framework designed to enhance and streamline the development of natural language processing (NLP) applications. By leveraging *LangChain's* capabilities, Langsmith simplifies the process of chaining multiple language models and other NLP components to build powerful applications. By integrating LangChain, Langsmith allows developers to create intricate NLP workflows that can perform various tasks, from text generation to sentiment analysis. Introduction to Langsmith Langsmith is an innovative framework designed to enhance and streamline the development of natural language processing (NLP) applications. It provides a framework for connecting language models to other data sources and interacting with various APIs. LangChain is designed to be easy to use, even for developers who are not familiar with lang 13 min read\", 'Get started with LangSmith | 🦜️🛠️ LangSmith Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. LangSmith + LangChain OSS LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application. Get started by creating your first prompt.', 'See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith Backend To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. LangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services. LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data). LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback). The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. LangSmith Backend LangSmith SDK', \"This paper provides an in-depth analysis of LangChain's architecture and core components, including LangGraph, LangServe, and LangSmith.\", 'This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications. From its seamless integration with LangChain to its robust Cookbook filled with real-world examples, LangSmith is a game-changer. Logging Assertions as Feedback (opens in a new tab): Convert CI test assertions into LangSmith feedback. Exporting LLM Runs and Feedback (opens in a new tab): Extract and interpret LangSmith LLM run data for various analytical platforms. While LangSmith is focused on building and managing LLM applications, LangChain serves as a framework for developing language models. LangSmith integrates seamlessly with LangChain, offering a unified platform for all your LLM needs.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain and Emerging AI Trends: Transforming the Future of AI Development By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. [Skip to main content](https://www.linkedin.com/pulse/langchain-emerging-ai-trends-transforming-future-sudeep-makwana-auawf#main-content) LangChain and Emerging AI Trends: Transforming the Future of AI Development Among the tools revolutionizing the AI landscape, LangChain, LangSmith, LangGraph, and LangFlow have emerged as pivotal frameworks for developers and researchers. LangSmith: A debugging and monitoring tool tailored for LangChain workflows, enabling developers to fine-tune and optimize their AI models in real-time. By abstracting the complexity of managing LLMs and integrating external systems, these tools accelerate the development of AI applications across industries like healthcare, finance, education, and e-commerce.', 'With the rise of generative AI, Infor saw an opportunity to future-proof its products by integrating LLMs into all its cloud suites via the Infor OS platform. With LangChain and LangGraph, the Infor engineering team was able to quickly implement a new GenAI component to the Infor OS platform to provide their various cloud suites and business applications access to LLMs. This helped them leverage rich industry knowledge and business cases to meet customers’ expectations for innovative business solutions. By integrating LLM-powered features throughout its cloud suite, Infor has streamlined report generation, automated content creation, and improved knowledge retrieval. By utilizing LangGraph and LangSmith, Infor is not merely adopting generative AI but redefining how enterprises interact with and benefit from AI-driven automation.']}}\n",
      "{'generate': {'draft': \"**Title: LangChain vs LangSmith: Exploring the Differences in AI Development**\\n\\nI. Introduction\\nLangChain and LangSmith are two innovative tools developed by LangChain, aimed at simplifying Large Language Model (LLM) applications. This essay will delve into the distinctions between LangChain and LangSmith to provide a comprehensive understanding of their unique features and applications.\\n\\nII. LangChain\\nLangChain is a Python framework designed for early-stage prototyping and small-scale LLM applications. It focuses on real-time data processing and seamless integration with large language models. LangChain simplifies AI development by streamlining data retrieval and processing, making it ideal for implementing the retrieval-augmented generation (RAG) pattern. For instance, developers can use LangChain to create prompt chains for chatbots or customize templates for specific applications. However, its limitations lie in scalability for large-scale production-ready applications.\\n\\nIII. LangSmith\\nIn contrast, LangSmith is a platform tailored for large-scale, production-ready LLM applications. It offers advanced debugging, testing, and monitoring capabilities, making it ideal for applications that require high performance and reliability. By integrating with LangChain, LangSmith enables developers to build intricate NLP workflows for tasks like text generation and sentiment analysis. While LangSmith excels in scalability and performance, it may be complex for early-stage prototyping due to its advanced features.\\n\\nIV. Comparison between LangChain and LangSmith\\nA. Technology Stack: LangChain focuses on streamlining AI application development, while LangSmith offers advanced observability features for monitoring and optimizing LLM applications.\\nB. Scalability: LangSmith is better suited for large-scale applications, whereas LangChain is more suitable for small-scale and prototyping projects.\\nC. Security: Both LangChain and LangSmith prioritize data security, but LangSmith's advanced monitoring capabilities enhance security measures for production-grade applications.\\nD. Interoperability: LangChain and LangSmith can be integrated to create comprehensive AI workflows, showcasing their interoperability.\\nE. Performance: LangSmith outshines LangChain in terms of performance, especially for complex and high-demand applications.\\n\\nV. Conclusion\\nIn conclusion, LangChain and LangSmith cater to different stages and requirements of LLM development, offering unique advantages and limitations. While LangChain is ideal for prototyping and small-scale applications, LangSmith excels in large-scale, production-ready projects. The integration of both tools can lead to robust AI applications with enhanced performance and reliability, shaping the future of AI development in various industries.\", 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9c544-3077-4bfc-acff-b42c88620544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

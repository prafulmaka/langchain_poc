{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef5a8fa-12c5-4961-9537-d9b3332ecebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:33:19.780403Z",
     "start_time": "2025-10-16T16:33:19.778770Z"
    }
   },
   "outputs": [],
   "source": [
    "# This if POC for testing LangChain implementation with Gymogul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c7938c-e37e-42c8-9ffc-010fc3e89437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "903c7116-a64a-4cdd-bc25-7b53bfb8e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a0d756-734e-407f-8504-16972b757c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "client = OpenAI(api_key=config[\"OPEN_AI_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb9a8ff-2109-4730-a110-0098fdacc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-mini\", model_provider=\"openai\", api_key=config[\"OPEN_AI_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfcd01d5-84f1-418b-bc93-8f5c54aef8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'finish_reason': 'stop', 'logprobs': None}, id='run--258fc923-09ed-4f07-bec0-9f8243200855-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7391a05-be01-43b8-973d-40cfb9e2b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I donâ€™t know your name based on our current conversation. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 11, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'finish_reason': 'stop', 'logprobs': None}, id='run--52f909d5-1adc-433f-a020-0bb3f5eea596-0', usage_metadata={'input_tokens': 11, 'output_tokens': 19, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What's my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10074069-5db1-4754-b6ce-a9690c545500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to your personal information unless you share it with me. What would you like me to call you?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 11, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'finish_reason': 'stop', 'logprobs': None}, id='run--a5225aa9-f01e-4204-9dba-06ef98520ed6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 24, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d44587a-5e37-4f45-ba88-3bd4b96b6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a SIMPLE example of how we can store chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a8046c3-136f-4878-8f77-846966fc73e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bob! I can analyze and interpret images you share with me, including individual frames extracted from a video. However, I don't have the capability to process video files directly or handle continuous video streams. If you extract specific frames from your video and upload them as images, I can certainly help analyze those! Let me know how you'd like to proceed.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 26, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'finish_reason': 'stop', 'logprobs': None}, id='run--717fb43c-f755-4b28-a423-7874a5310d35-0', usage_metadata={'input_tokens': 26, 'output_tokens': 70, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        # AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Are you able to process image frames from a video?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c8f046b-9461-4b98-95e0-7764e6dba6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how we can process images and use in LangChain messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aea7c79-7adb-4358-91b2-386827197708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame=0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    return base64Frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab9fd82-a75f-4ef3-aca6-1594dd67796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 21 frames\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import base64\n",
    "video_path = \"pushup.MOV\"\n",
    "base64Frames = process_video(video_path, seconds_per_frame=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526d065a-470d-4187-aad3-aabc58c4a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are generating a video summary. Please provide a summary of the video. Respond in Markdown.\"),\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Please analyze the frames from this video and tell me what is happening.\"},\n",
    "            *[\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame}\"},\n",
    "                }\n",
    "                for frame in base64Frames\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7d9092-7767-4f72-b6b5-a18e8f9a08d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"These frames depict a sequence where a person performs an exercise or workout routine involving a transition from standing to a crawling-like position and then moving into a plank or push-up position. Here's what is happening step-by-step:\\n\\n1. The person starts in a standing position with knees bent.\\n2. They prepare for movement by slightly leaning forward.\\n3. They bend down further, placing their hands closer to the floor.\\n4. The person places their hands on the floor while maintaining a bent-knee position.\\n5-11. The person shifts their weight forward, moving the body into a plank position, possibly transitioning between a crouch and a push-up/plank.\\n12-14. The person appears to be in a full plank or push-up position.\\n15. The person shifts back or finishes the plank position.\\n16-17. The person moves back to a crouched position.\\n18-19. The person stands back up and faces the camera with a slight smile.\\n\\nOverall, it looks like an exercise involving dynamic movement from standing to crawling/plank and back, perhaps to build strength or practice a specific calisthenics move. The final frame showing a smile indicates the person might be wrapping up or pleased with their performance.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 51368, 'total_tokens': 51614, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'finish_reason': 'stop', 'logprobs': None}, id='run--66531a51-dc39-418e-b4cc-b54b636bc729-0', usage_metadata={'input_tokens': 51368, 'output_tokens': 246, 'total_tokens': 51614, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f63529-6361-4c24-90c4-909d9235391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can try this approach to remove any heavy HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19483a2e-7537-453f-b19f-02e59f7d769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are generating a video summary. Please provide a summary of the video. Respond in Markdown.\"),\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Please analyze the frames from this video and tell me what is happening.\"},\n",
    "            *[\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame}\"},\n",
    "                }\n",
    "                for frame in base64Frames\n",
    "            ]\n",
    "        ]\n",
    "    ),\n",
    "    HumanMessage(content=\"My name is Allison. What is your name?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31ed428e-fa0e-4dcf-a87b-26cd0ba4b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the heavy image message\n",
    "for msg in messages:\n",
    "    if isinstance(msg.content, list):\n",
    "        messages.remove(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99558c22-8778-48fd-b199-38f8a1ddd5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
